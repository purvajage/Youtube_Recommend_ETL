version: "3.9"


services:

  # MySQL
  de_mysql:
    image: mysql:8.0
    container_name: de_mysql
    volumes:
      - ./mysql:/var/lib/mysql
      - ./dataset/youTube_trending_video:/tmp/youTube_trending_video
      - ./load_dataset:/tmp/load_dataset
    ports:
      - 3306:3306
    env_file: .env
    networks:
      - de_network

  # MinIO
  minio:
    hostname: minio
    image: minio/minio
    container_name: minio
    ports:
      - 9001:9001
      - 9000:9000
    command: [ "server", "/data", "--console-address", ":9001" ]
    volumes:
      - ./minio:/data
    env_file: .env
    networks:
      - de_network

  mc:
    image: minio/mc
    container_name: mc
    hostname: mc
    env_file: .env
    entrypoint: >
      /bin/sh -c " until (/usr/bin/mc config host add minio
      http://minio:9000 minio minio123) do echo '...waiting...' && sleep 1;
      done; /usr/bin/mc mb minio/lakehouse; /usr/bin/mc policy set public
      minio/lakehouse; exit 0; "
    depends_on:
      - minio
    networks:
      - de_network

  # Pipeline
  etl_pipeline: 
    build:
      context: ./etl_pipeline
      dockerfile: Dockerfile
    container_name: etl_pipeline
    image: etl_pipeline:latest
    restart: always
    volumes:
      - ./etl_pipeline:/opt/dagster/app
      - ./docker-images/spark/spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf
    ports:
      - 4041:4040
    env_file: .env
    networks:
      - de_network

  # PostgreSQL
  de_psql:
    image: postgres:15
    container_name: de_psql
    volumes:
      - ./postgresql:/var/lib/postgresql/data
      - ./load_dataset:/tmp/load_dataset
    ports:
      - 5432:5432
    env_file: .env
    networks:
      - de_network

  # Dagster
  de_dagster:
    build:
      context: ./docker-images/dagster/
    container_name: de_dagster
    image: de_dagster

  de_dagster_dagit:
    image: de_dagster:latest
    entrypoint:
      - dagit
      - -h
      - "0.0.0.0"
      - -p
      - "3001"
      - -w
      - workspace.yaml
    container_name: de_dagster_dagit
    expose:
      - "3001"
    ports:
      - 3001:3001
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
    env_file: .env
    networks:
      - de_network

  de_dagster_daemon:
    image: de_dagster:latest
    entrypoint:
      - dagster-daemon
      - run
    container_name: de_dagster_daemon
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
    env_file: .env
    networks:
      - de_network

  # Streamlit
  de_streamlit:
    build:
      context: ./docker-images/streamlit
      dockerfile: Dockerfile
    image: de_streamlit:latest
    container_name: de_streamlit
    volumes:
      - ./app:/app
    env_file: .env
    ports:
      - "8501:8501"
    networks:
      - de_network

  # Metabase
  de_metabase:
    image: metabase/metabase:latest
    container_name: de_metabase
    volumes:
      - ./storage/metabase_data:/metabase_data
    ports:
      - "3030:3000"
    env_file: .env
    networks:
      - de_network

  # Jupyter
  # de_notebook:
  #   image: jupyter/all-spark-notebook:python-3.9
  #   container_name: de_notebook
  #   command: [ "start-notebook.sh", "--NotebookApp.token=" ]
  #   ports:
  #     - 8888:8888
  #   volumes:
  #     - ./notebooks/work:/home/jovyan/work
  #   env_file: .env
  #   networks:
  #     - de_network
      
  # # Spark
  # spark-master:
  #   build:
  #     context: ./docker-images/spark
  #     dockerfile: Dockerfile
  #   image: spark-master:latest
  #   container_name: spark-master
  #   hostname: spark_master
  #   volumes:
  #     - ./docker-images/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
  #     - ./data:/opt/spark-data
  #   env_file: .env.spark_master
  #   expose:
  #     - "7077"
  #   ports:
  #     - "7077:7077"
  #     - "8080:8080"
  #   networks:
  #     - de_network

  # spark-worker:
  #   image: docker.io/bitnami/spark:3.4.3
  #   depends_on: 
  #     - spark-master
  #   deploy: 
  #     replicas: 3
  #   env_file: .env.spark_worker
  #   volumes:
  #     - ./docker-images/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
  #     - ./data:/opt/spark-data
  #   networks:
  #     - de_network

networks:
  de_network:
    driver: bridge
    name: de_network